---
layout: post
title:  "数据科学导论 chapter4"
categories: Data_Science
permalink: :/categories/:day/:month:/year
---

### Case study

#### Netflix

- Data sources: user rate, click, profile
- Data volume: 25 million users,m 4 million Tates/day, 3 million search/day, video cloud of 2 petabytes
- Data velocity:  video title/ranking/rating change daily
- Data variety: user ranking, user profiles, media properties
- Software: Hadoop, Cassndra, Teradata
- Analytics: personalised recommender system
- Processing: analytic processing, streaming video
- Capabilities: rating search, content delivery
- Security/privacy: protect user data, digital right
- Lifecycle: continued ranking and updating

### Application areas

- Health

- Government 

- Retail 

- Manufacturing 

### Characterising Data

#### Metadata

metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use or manage an information resource.

-   Descriptive

-   Structural
    
-   Administrative
    
#### Why we use metadata

-   Information retrieval and dissemination
    
-   Resource description
    
-   preservation and retention
    
-   rights management
    

#### Key concept

-   Machine-readable data
    
-   markup language
    
-   digital container
    

#### Type of data

-   Geospatial data
    
-   Linked open data
    
-   Transitional data
    
-   Social media data
    

### Evolving law

#### Moore's Law

Moore's law states that the transistors in a dense integrated circuit double approximately every two years(starting from 1975).

#### Koomey's Law

Koomey's law states that the energies efficiency doubles roughly every 18 months.

#### Bell's Law

Roughly every decade a new, lower-priced computer class forms based on a new programming platform, network, and interface resulting in new usage and the establishment of a new industry.

#### Zimmerman's Law

Zimmerman's Law states that the natural flow of technology tends to move in the direction of making surveillance easier, and the ability of computers to track us doubles every eighteen months.

### Big data processing

#### Business context

Businesses function in a continuously changing environment:

- Fixed formats as per RDBMS, not suitable
- Usage varies, requires complex analytical queries

Need to reach insights faster and act on them in real-time

- Stream processing

#### Database background concept

- In-database analytics

  The analytics is done within the DB 

- In-memory database

  The DB content resides in memory

- Cache

  Data stored in-memory
  Key-value
  Value accessible by key (eg: hash table)

- Information silo
  An insular information system is incapable of reciprocal operation with other, related information systems

### Databases 

RDBMS relational database management systems

SQL structured query language

#### Old Database limit

In general, data science is not compatible with the RDBMS schema and type constraints:

- RDBMS are bad at storing unstructured or semi-structured data.
- Rigid schemas mean RDBMS are expensive to set up and maintain.
- Must know your use cases inadvance.
-  Changing the schema is difficult and time-consuming.
-  Slow at processing that data.

#### New Database

Many NoSQL and SQL DBs offer the following features:

- Large scale, distributed processing
- Robustness achieved
- General query languages
- Some notion of consistency

#### JASON

(**J**ava**S**cript **O**bject **N**otation)

- No fixed format
- Semi-structured, key-value pairs, hierarchical
- “Friendly” alternative to XML
- Self-documenting structure
- JSON is a syntax for storing and exchanging data.
- JSON is text, written with JavaScriptobject notation.

#### When to choose

**Use SQL database when**

- Data is structured
- Data is unchanging

 **Use NoSQL database when**

- Stores a large volume of data with little structure
- Data changes rapidly

#### Graph Database

- Data is represented as a network of interconnected nodes (objects) and enable visualisations and graph analytics e.g. Citation network analysis and social network analysis
- stores graph, commonly as triples: subject, verb, object
- Commonly used to store Linked Open Data

### Distributed Processing

#### Breaking up computation

- Interactive
- Streaming
- Batch

#### Cluster

Networks of computers are connected and work cooperatively to achieve the same task. This is a cluster.

Clusters improve:

- Availability
- Scalability
- Performance
- Efficiency
- Costs

### Hadoop

- Based on map-reduce architecture 

- suitabble for offline processing


### Apache spark

- Includes map-reduce capabilities

- Provides real-time, in-memory processing

- faster than Hadoop
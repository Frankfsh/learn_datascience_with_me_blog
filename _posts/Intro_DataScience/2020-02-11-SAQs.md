## Set A SAQs
1. What is a clinical trial and why are they used? Give an example.

   They are experiments ("trials") done in medicine ("clinical research") to answer a specific question about a treatment. Usually they follow the scientific method: subjects are randomly selected for "treatment" or placebo, and the treatment is done blind so that subjects cannot know which they get. At the end of the trial results are recorded and analysed. Example:  have 1000 elderly men take daily aspirin or placebo for a year, and record their blood pressure and cholesterol before and after to answer the question, "does daily aspirin support cardiac health". 

2. Give an example of a Machine Learning Platform and explain why it is used.

   (Note we didn't ask you about a specific platform, you could pick the one you knew) TensorFlow is an open source library for ease of computational deployment of numeric algorithms to GPUs and CPUs, originally developed by Google. It is used by machine learning researchers wanting to port their numerical algorithm onto GPUs without them requiring specific knowledge of the hardware and its coding.

3. What is a scripting language, and what is their relationship to rapid prototyping?

   See slide 25. Scripting languages are ideal for rapid prototyping, so are often used for it.

4. What are the problems with Google Flu trends?

   The Google team used specific search queries as proxy data for the existence of flu. These were not disclosed, so the system wasn't open.  This also becomes very subject to public perception:  news reports could make people go searching for flu even if they didn't have it. Also flu and flu-like diseases are hard to distinguish by the non-expert. Finally, Google didn't use standard time series approaches which would have used the CDCs reported data as well in the prediction.

5. Describe two kinds of problems that can arise with the application of the scientific method, and give examples.

   See slide 35.


## Set B SAQs
These should be answered on paper and discussed during tutorial.
Write 3-7 lines on each of the following.

1. Consider the definition given for data science. Keep in mind that the boundary between data science, data engineering and data analysis is somewhat fluid.  **What is the importance of the definition?**

   Definition is *like a mission statement* (1). *So educators have something to focus on (1)*, and *employers know they have the right job descriptions* (1), etc. It is fluid though ,... there is no hard and fast answer unless the American Association of Data Scientists becomes a government mandated monopoly. The definitions are gradually coalescing as industry and practitioners find out what works. Importantly, distinctions need to be made with "near" communities like business analytics and data engineering. There are big overlaps in job roles and functions in practice though.

2. Briefly review the two slides on the car industry. Note that first they underwent a digitisation process, followed by a datafication process. **Give two**  other non-automotive industries that have had similar developments in recent decades. **How do you expect this to change**  these industries?

   *aircraft engine manufacturers*: more effective data on lifetime of engine, better support for maintenance and design, better scheduling of maintenance
   *supermarkets*: tracking of products means better knowledge of inventory and lost items; tracking users (around store, as well as purchases) means placement of items, better marketing, better targeted sales 
   *software vendors* (Microsoft and many others now "phone home" with data): while we know its used for tracking HCI issues, and maintenance updates, there are probably reasons concerned with modifying/optimising billing practices, going to SaaS etc.

3. **What role has the internet had**  in the development of data science?

   This is all over the various initial lectures:  *internet giants were the first big users of data science* (1), making it visible to the broader business community; *internet-driven social networks and commerce sites provided rich varieties of data for use*;(1); open data sources, tools and training all up on the internet; really *the internet was the incubator for data science* (1).

## Set C SAQs

1. Name a popular Apache open source product for big data (but not Hadoop or HDFS), (A) briefly describe what is does, and (B) give a short use case for a data science project

   See slide 12 of this week.  Use case is about 3 lines describing application.

2. Name a popular data/information API, (A) briefly describe what is does, and (B) give a short use case for a data science project

   See slide 32.

3. Figure-Eight provides human-in-the-loop tools and resources for dataset generation and staffing. Find their Human-in-the-loop FAQ and research it briefly. Then: (A) why would one use human-in-the-loop methods to build a training set and (B) give a short use case

   NOTE: this could not be in the exam unless we listed the relevant parts of their FAQ somewhere. Now predictive modelling requires high quality data, and the more the better. So if there isn't the data available, one needs to acquire it somehow, and one way to do it is to get labels from available experts. Also, existing data may be poorly labelled, in which case you would like the labels checked. Now one cannot have experts label some things like which customers defaulted on their bank loan, but they usually can label things like images, properties of text data, and so forth. For use cases see their "Success stories" web page.

## Set D SAQs

1. Give an example of data with a problem with veracity, and discuss the problems it causes. Remember, well understood measurement error (e.g., only recording to 3 decimal places) is not usually considered to be veracity.

   Generally systematical or predictable errors are not considered wrongful data. They are those tempered with or containing distorted signals. E.g., ozone hole example is NASA; in medical, wrong diagnostic codes by human error; diet data is intrinsically errorful; in industries, such as instrument break downs. 

2. Explain how Koomey's Law has affected data science.

   Koomey's Law means instruments can be used to gather data in low energy context. In the 2000s that led to the rise of mobile smart phones whioch helped kick off the social web that creates a lot of the data used in data science. Second, it has more recently led to the rise of the internet of things, where data can be sourced from many different small devices: small cameras around the city, standard household devices giving their status, instrumentation embedded in industrial plants.

3. Give a recent/future example of Bell's Law, and discuss the kind of new data it provides.

   *Internet of things* is one example, where data can be sourced from many different small devices. This can provide status data on devices as well as monitoring data of devices, temperature in the fridge, engine runtime characteristics, etc.

## Set E SAQs

1. Give an example where two very different data sets needed to be combined in order to make a data science project work.

   Many listed in first part of talk: web pharmacovigilance (2nd source is FAERS, combined gives better results); traffic prediction (weather, events, historical traffic) 

2. Define what proxy data is, and give an example of its use.

   See slide 19.  Term used by paleoclimatologists but means data that is used as a substitute for an unmeasurable variable. We expect it to be highly correlated.  Example is (1) search query data about health to indicate the incidence of the terms being queried, (2) crash data on intersections as proxy for "danger to cyclists"

3. One example of data wrangling is extract dates from text and converting them to a digitised date format. What sorts of problems are encountered here? What sorts of software is available to deal with it?

   See slide 32.

4. Give an example of a data standard that is in use: name the standard, give its domain, the sort of data it supports, and why it is used.

   See slide 36: ICD-10, SnoMedCT, Dublin Core, NewsML

## Set F SAQs

1. Present two different kinds of tasks: (a) one which is purely ("embarassingly") data parallel, i.e., very suited for Hadoop, and (b) one which would be very hard to make data parallel. For each, explain why it works or doesn't.

   Present two different kinds of tasks: (a) one which is purely ("embarassingly") data parallel, i.e., very suited for Hadoop, and (b) one which would be very hard to make data parallel. For each, explain why it works or doesn't.
   ==>> Standard examples for (a) are turning document collection into an inverted index; typical image/doc processing applications (where each is done independently), such as "find faces"; looking for common 3-4 word phrases in a collection; and for (b) are optimisation or global tasks such as compute the top principle component(s) of a large graph, invert a large matrix, design the layout for a computer chip based on a computerised circuit diagram.

2. Consider a graph database, such as DBpedia. Give an example of a commercial or government application that would use a graph database, and discuss why it is appropriate.

   Consider a graph database, such as DBpedia. Give an example of a commercial or government application that would use a graph database, and discuss why it is appropriate.
   ==>> New York Times keeps graphDB linking news articles to people, places and events to support the fact checkers and writers, best way for them to access things; Google has a similar graphDB, much larger, though we are unsure of usage and how it relates to data science

3. Give an example from the healthcare/medical domain where good data collection followed by analysis has (or could) led to improvement patient outcomes. Briefly explain the (1) data collected, the (2) analysis done and (3) the improved outcomes.

   Give an example from the healthcare/medical domain where good data collection followed by analysis has (or could) led to improvement patient outcomes. Briefly explain the (1) data collected, the (2) analysis done and (3) the improved outcomes.

## Set G SAQs

1. Peter Norvig talked about the "unreasonable effectiveness of data". What does he mean by that?

   This comes from the term "unreasonable effectiveness of mathematics" which is claimed of Physics and Engineering where a smaller number of mathematical principles are the basis for large parts of theory. This is not the case in areas like health, bioinformatics, sociology, economics where Norvig claims instead data can be used to answer many questions.

2. Give an example of "customer segmentation" from industry. Describe the kind of data used to build the segmentation, and the what the segmentation is used for.

   Should be a realistic example ... doesn't have to be confirmed truth). A search engine or internet company (Yahoo, Yandex, ...) might "bucket" their users into 500 different groups, based on their web interactions (search queries, advertisement responses, purchases). This is then used as a "grouping" to support which advertisements and news will be show to the user on their home page or during other activities. So now, each group will have its own advertisement and news predictions, instead of the company having to create predictions personally for every single user.

3. Describe what bias and variance are.

   See slide 40. Examples on slide 39, 41.

4. Describe what ensembles are.

    See slide 49.

## Set H SAQs

1. Consider the video by Foster Provost we saw in Lecture 3. Now look at the NIST analysis on page 8 of the slides for Lecture 4. Name two issues that Foster touched on and why they were relevant to his application.

   - data sources: they have account and financial transaction data; note they also used a lot of other features that probably had been extracted from external sources like socioeconomic data
   - data variety: bringing in the fine-grained data of credit card transactions allowed the predictions to improve; note it made the prediction more complex; 
   - data volume: they were getting this for upto 1 million consumers so we can guess data is in the 10-100Gb (i.e., 10-100k per consumer)

2. Consider the NYT article, "Lord Mayor's Geek Squad" discussed in this week's lecture. Name two issues from the NIST analysis touched on and why they are relevant to this application.

   - data sources: the breadth of data available from the city, mostly relational tables
   - data volume: most datasets seem to be quite small: Mbs but maybe Gbs for parking data
   - analytics: NYC contains no one single application, rather whatever people can do, so many different kinds of analytics could be done

3. Give two examples of Data Science applications specifically in retail (selling mass produced goods to consumers).

   several in marketing: cross-sell for websites; sentiment analysis to ascertain what consumers like; customer segmentation to support advertising campaigns


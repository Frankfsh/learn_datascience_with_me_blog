# week1 learning outcome

## data science

data science is the emprical sythesis of  actionable kownledge from raw data through the complete data lifecycle process.

## data scientist 

data scientist is a partictioner who has sufficient knowledge in the overlapping regimes of business need, domain knowledge, analytical skills, software and system engnieering to manage the end-to-end data process through each stage of data lifecycle.

## data and decision models

### value chain

sequence or processes of data science

- collection
- wrangling
- preseantation
- engineering
- governace
- operationlisation

### analytic levels

descriptive terms used to broadly classify kinds of analysis

- descriptive analytics
  gain insight fron historical data
- predictive analytics
  make prediction using statistical and machine learning techniques
- prescriptive analytics
  recommend decision using optimization and simulation

## business model 

business model describes how an organization creates, delivers, and captures the value in economic, social, cultural or other contexts.

### tranditional IT business model

 - software as a service
 - consulting
 - customer relationship mangement

### data business model

 - info brokering
 - info-based differentiation
 - info-based delivery network
 - info provider

### case study

- Bloomberg Terminal
  info brokering
  allow finance professionals to monitor and analyse real-time fiancial market data
- Amazon
  info-based differentiation & info-based delivery network
  statisfies customers by providing a differentiated service and delivering information
- LexisNexis
  info-provider
  provdie  electronic database for legal and public-records related information 

## Data science role and skills

### Data scinece role 

- Data scientists

  people use their analytical and technical capabilities to extract insight from data that can be apllied in business or social context.

- Data engineering

  people manage infrastructure, automate data processing and deploy models at scale.

- Data analysts

  People who understand the statistical theory and the use it to solve practical problem of various industries.

### Data scientist types difference

- Data scientists are primarily people who develop models and products in turn produce insight.
- Data analysts are primarily people who develop insight with data.
- Data engineers are primarily people who mange data infrastructure, process and deploy models at scale.

### Data science skills

- business processes
- machien learning/Big data
- Mathematics/Operations research
- Programming
- Statistics

## Impact of data science

### Impact of life

#### Advantage

- Personal agency
- Health
- Convenience
- Communication 

#### Disadvantage

- Targeting base on vulerability
- Missuse of personal infromation
- Algorithm discrimation
- Data breeches and theft
- Election interference
- Social manipulation
- Big data blocklist

### Impact of sicence

- independent fields
- government interest
- new data sources
- more inclusive

### Impact of social

- suicide prevention
- fighting child exploitation
- managing natural disasters

### Impact of future

- Healthcare
- Automotive

## Pros and Cons for motion chart

### Advantages

- time dimension for trend observation

- good for exploratory data analysis

- allows to identify non_common "rhythm"

- appeal to the brain at a more instinctual intuitive level


### Disadvantage

- not suited for static media

- complex display

- not suited for representing all data

# week2 learning outcome

## NIST big data analytics frame

- Data sources
- Data volume
- Data velocity
- Data variety
- Software
- Analytics
- Processing
- Capabilities
- Security/privacy
- Lifecycle

## Characterising data

### 4 V's

- velocity
  the rate at which data is being 
  generated/collected/analysed
- volume
  the volume is how much data is being generated/collected/analysed
- variety
  the breadth of the types of data
- varacity
  the variety of data concerns how reliable and significant it really is

### Metadata

#### Definition

metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use or manage an information resource.

-   Descriptive
    metadata that describe content for identification and retrieval
-   Structural
    metadata that document the relationships and links between data
-   Administrative
    metadata that help manage infromation

#### Why use ?

- information retrieval and dissemination
- resource description
- preservation and retention
- rights management

#### Key concept

- machine-readable data
  format that can be understood by a computer
  e.g., XML,JSON,RDF
- markup language
  annotating a document in a way that is syntatically distinguishable from text
  e.g., Markdown, javadoc
- digital container
  file format whose specification describes how different elements coexist.

### Type of data

- Structured data
  Structured data is data whose elements are addressable for effective analysis. It has been organized into a database SQL.
  Example: Relational data
- Semi-Structured data
  Semi-structured data is information that does not reside in a relational database but that have some organizational properties that make it easier to analyze. 
  Example:  CSV, XML , JASON.
- Unstructured data
  Unstructured data is a data that is which is not organized in a pre-defined manner or does not have a pre-defined data model. 
  Example: Word, PDF, Text, Media logs.

## Evolving law

- Moore's Law
  Moore's law states that the transistors in a dense integrated circuit double approximately every two years(starting from 1975).
- Koomey's Law
  Koomey's law states that the energies efficiency doubles roughly every 18 months.
- Bell's Law
  Roughly every decade a new, lower-priced computer class forms based on a new programming platform, network, and interface resulting in new usage and the establishment of a new industry.
- Zimmerman's Law
  Zimmerman's Law states that the natural flow of technology tends to move in the direction of making surveillance easier, and the ability of computers to track us doubles every eighteen months.

## Database

### Legacy databases

- flat-file database
- hierarchical database
- network database

### Relational database

Relational databases organize data using *tables*. Tables are structures that impose a schema on the records that they hold. Each column within a table has a *name* and a *data type*. Each row represents an individual record or data item within the table, which contains values for each of the columns.

### NoSQL database

Modern alternatives for data that doesn't fit the relational paradigm.

- Key-value databases/key-value stores

  simple, dictionary-style lookups for basic storage and retrieval

  e.g. [Redis](https://redis.io/)

- Document databases

  Storing all of an item's data in flexible, self-describing structures
  
  e.g. [MongoDB](https://www.mongodb.com/)
  
- Graph databases

  mapping relationships by focusing on how connections between data are meaningful

  e.g. [Neo4j](https://neo4j.com/)

- Column-family databases

  databases with flexible columns to bridge the gap between relational and document databases

  e.g. [Cassandra](https://cassandra.apache.org/)

## Distributed processing

breaking up computation

- interactive
- streaming
- batch

### Cluster

Networks of computers are connected and work cooperatively to achieve the same task. This is a cluster.

Clusters improve:

- Availability
- Scalability
- Performance
- Efficiency
- Costs

### Hadoop

- Based on map-reduce architecture 
- suitabble for offline processing, not suitable for streaming
- 3 commonents of Hadoop:
  - Hadoop Distributed File System(HDFS)
  - MapReduce
  - Yet Another Resource Negotiator(YARN)


### Spark

- Includes map-reduce capabilities
- Provides real-time, in-memory processing
- faster than Hadoop



# week3 learning outcome

## lessond from example

- NYC data
  - Data requires work to clean up
  - Be creatibe about sources
  - Getting on the fount foot and combing resources is best
- traffic prediction
  - combine many sources
- Predictive analytics for bank
  - fine-frained data helps, but it is harder to use

## Open data

publicly available

Government and IT departemnts building data and infrastructure to allow sharing

machine readable

##  Data wrangling

Process of transforming "raw" data into data that can be analysed to generate valid actionable and insights.

### Needs and importance

- Data comes in all shapes and sizes
- different files have different formatting
- mistakes in data entries
- digital preservation

### more wrangling task

- integrate data sources
- geocoding
- covert free text to standard format
- deal with missing value
- deal with average value

### types of erros

- Syntactic anomalies are erros in data format
- Semantic anomalies include inconsistencies
- Coverage anomalies refer to missing data

## Data standards

### Cross industry standard for data mining(CRISP-DM)

- Business understanding
- Data understanding
- Data preparation
- Modelling
- Evaluation
- Deployment

### Semi- structured data

XML, JSON, YAML

### Model language

Predictive model markup language (PMML) provides a standard language for describing a model that can be passed between analytic software

### Open source software

TensorFlow

Tensorwatch

### Data science tool

### REST API and SaaS

- REST: representational state transfer
- API: applocation programmer interface
- SaaS: software as a service



## Data privacy

#### Privacy

Privacy is a human right underpinning freedom of association, thought and expression, as well as freedom from discrimination.

#### Personal information

Personal information includes a broad range of information, or an opinion that could identify an individual.

#### Sensitive information

Sensitive information is personal information that includes information or an opinion about an individual's 

#### Confidentiality

Confidentiality as information privacy, how information about an individual is treated  and shared.

#### Data security

Security as the protection of data, preventing it from being improperly used.

#### Data breach

A data breach happens when personal information is accessed or disclosed without authorisation or is lost.

#### Implicit data

Any data points or information that an individual does actively and intentionally provides.

#### Explicit data

Any data points or information that an individual does not actively provide but is instead gathered from data streams or is generated through analysis of explicit data.

#### Value

The fundation of your ability to judge right or wrong, which are deeply linked to your system of belief.

#### Morals

Morals are formed out of values. They are the system of beliefs that emerge out of a person's core values.

#### Ethics

Ethics are the vehicle to our morals, or our morals in action. Ethics enact the system we've developed in our moral code.

## Week4 outcome

### Data Management

Data management is the development, execution and supervision of plans, polices, programs and practices that control, protect, deliver and enheance the value of data and information assets.

Data management ensures that the data are trusted, reusable, interoperable, findable, accessible.

#### Importance of data management

- porductivity
- cost efficiency
- opertional nimbleness
- Security risks
- reduced data loss
- accurate decisions

### Data governance

Data governance refers to the governing of cross-border data flows by countries, and Hance is more precisely International data governance. This newfound consists of "norms principles and rules governing various types of data".

####  Supporting and handing:

- ethics, confidentiality and security
- consolidation and quality-assurance
- presistence
- Regulatory compliance

#### Benefits

- Lower costs associated with data management
- more accurate procedures around regulations and compliance activities
- More transparency within any data-related activities and better resolution of known data issues
- Better training and education
- Increase in value of business data
- provide standardised data system
- improved monitoring and tracking capabilities

#### Auditing

Auditing systematic and independent examination of books, accounts, documents and vouchers of an organization to ascertain how far they present a true and fair view.

- Ethics, condidentiality
- securtiy
- consolixation and quality-assurance
- persistence 
- regulatory
- Regulatory complinace
- organisation policy compliance

### Metadata

#### Purpose of metadata

- endables research datasets to be discovered
- enables organisational data to be efficiently retrieved and accessed
- reduce data breeches
- give detailed contextual information
- make aggregation possible
- determine data
- used for mechine learning

#### Metadata schemas

a schemas is a logical plan showing the relationship between metadata elements.

#### Metadata Standards

- Structure standards: ensure consistent structure for data sharing and searching, managing the creation of the data, establishing record provenance and technical processing and managing whao can access the data.
- Content standards: ensure content is machine readable.



#### Dublin core

Dublin core is a general metadata standard which expressed as XML has 15 elements that make up the basic descriptive, administrative and technical elements needed to be identified datasets. It can assist interoperability between models.

XML has a flexiable structure allowing full conversion of data from one metadata standard to another.



### Data curation

Data curation is an end-to-end process of creating good data through the identification and formation of resources with long-term value.

#### DCC

digital curation life cycle

the DCC curation lifecycle model outlines 

- data(digital objects or database)
  - digital objects
  - database
- full lifecycle
  - description and representation information
  - preservation planning
  - community watch and participation
  - curate an preserve
- sequential actions
  - conceptualise
  - create  or receive
  - appraise and select
  - ingest
  - preservation action
  - sore
  - access, use and reuse
  - transform
- occasional actions
  - dispose
  - reappraise
  - migrate



#### DataONE

- plan
- collect
- assure
- describe
- preserve
- discover
- Integrate
- analyse



#### Why curate?

- make ML more effective
- Mopping up data swaps
- Increasing the pace innovation



#### Data Management Plans

- Security
- Sharing and disseminating data
- Organising data
- Re-use



#### Data Management for Data Science

- Medical informatics

- Internet advertising

- Predictive medical system



### Prototype  and scripting language

#### Rapid prototyping

the activity of creating prototypes of software application, incomplete version being developed.

#### Benefits:

The designer and implementer can get valuable feedback and the cilent and contractor can compare if the software matches the specification early in the project.

It allows insight into the accuracy and whether the deadlines and milestone can be met.

#### scripting language

a scripting language is a programming language for a specail run-time environment that automates the execution of task, or alternatively be excuted one-by-one by a human operator.

### probability

#### independent and dependent

- independent

  the outcome of one even dont effect the outcome of others.

- dependent

  the outcome of one event affects the outcome of the others.

  

### correlation vs causlation

- correlation

  a statical relationship between two random variables.

- causation

  a realationship between an event(the casue) and another event(the effect), where the cause is responsible for the effect.

- causal learning

  understand why a variables is in its current state and predict new variables.

- causal inference

  Predict the value of a variable after some other variables has been manipulated.

- Randomized experiment

  a mainipulation to set the value of a variable in place of the naturally occuring mechanism that determines the variable's value. Clinical trails are randomized expertiments.

- Observational studies

  Predict what the effects of mainipulationg would be.

  diagnostic test, treatment or other intervention

  - Cohort study

    Compare cohort with different exposures.

  - case control study

    Compare cases with or without controls from respect of exposure.

- Correlation dose not imply causation

### controlled experiments

Types of varaiables

- Independent
- Dependent
- Controlled

## Randomised control trais(RCT)

Aim to redue bias

### Main types

- paralle
- crossover
- cluster
- factorial

### Disadvantage and advantage

- disadvantage
  - expensive
  - slow labor intensive
  - not ethical

- advantage
  - eliminate bias
  - blinding participants
  - apply probability theory



### significance testing primer& caution

- don't repeat until successful
- record negative results
- ensure repeatability by recording methdology and processing



### False Positive rates

rate of true positives = TP/(TP+FN)

rate of false positives = TN/(TN+FP)

### ROC curve

the curve plot the false positive and true positive

the best classficaiton has the largest area under curve
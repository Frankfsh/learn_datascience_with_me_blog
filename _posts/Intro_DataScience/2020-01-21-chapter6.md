# week1 learning outcome

## data science

data science is the emprical sythesis of  actionable kownledge from raw data through the complete data lifecycle process.

## data scientist 

data scientist is a partictioner who has sufficient knowledge in the overlapping regimes of business need, domain knowledge, analytical skills, software and system engnieering to manage the end-to-end data process through each stage of data lifecycle.

## data and decision models

### value chain

sequence or processes of data science

- collection
- wrangling
- preseantation
- engineering
- governace
- operationlisation

![standrad_value_chain.png](https://github.com/Frankfsh/learn_datascience_with_me_blog/blob/gh-pages/images/Intro_DataScience/standrad_value_chain.png?raw=true)

### analytic levels

descriptive terms used to broadly classify kinds of analysis

- descriptive analytics
  gain insight fron historical data
- predictive analytics
  make prediction using statistical and machine learning techniques
- prescriptive analytics
  recommend decision using optimization and simulation

## business model 

### tranditional IT business model

 - software as a service
 - consulting
 - customer relationship mangement

### data business model

 - info brokering
 - info-based differentiation
 - info-based delivery network
 - info provider

### case study

- Bloomberg Terminal
  info brokering
  allow finance professionals to monitor and analyse real-time fiancial market data
- Amazon
  info-based differentiation & info-based delivery network
  statisfies customers by providing a differentiated service and delivering information
- LexisNexis
  info-provider
  provdie  electronic database for legal and public-records related information 

# week2 learning outcome

## characterising data

### 4 V's

- velocity
  the rate at which data is being 
  generated/collected/analysed
- volume
  the volume is how much data is being generated/collected/analysed
- variety
  the breadth of the types of data
- varacity
  the variety of data concerns how reliable and significant it really is

### metadata

#### definition

metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use or manage an information resource.

-   Descriptive
    metadata that describe content for identification and retrieval
-   Structural
    metadata that document the relationships and links between data
-   Administrative
    metadata that help manage infromation

#### why use ?

- information retrieval and dissemination
- resource description
- preservation and retention
- rights management

#### key concept

- machine-readable data
  format that can be understood by a computer
  e.g., XML,JSON,RDF
- markup language
  annotating a document in a way that is syntatically distinguishable from text
  e.g., Markdown, javadoc
- digital container
  file format whose specification describes how different elements coexist.

### Type of data

- Structured data
  Structured data is data whose elements are addressable for effective analysis. It has been organized into a database SQL.
  Example: Relational data
- Semi-Structured data
  Semi-structured data is information that does not reside in a relational database but that have some organizational properties that make it easier to analyze. 
  Example:  CSV, XML , JASON.
- Unstructured data
  Unstructured data is a data that is which is not organized in a pre-defined manner or does not have a pre-defined data model. 
  Example: Word, PDF, Text, Media logs.

## Evolving law

- Moore's Law
  Moore's law states that the transistors in a dense integrated circuit double approximately every two years(starting from 1975).
- Koomey's Law
  Koomey's law states that the energies efficiency doubles roughly every 18 months.
- Bell's Law
  Roughly every decade a new, lower-priced computer class forms based on a new programming platform, network, and interface resulting in new usage and the establishment of a new industry.
- Zimmerman's Law
  Zimmerman's Law states that the natural flow of technology tends to move in the direction of making surveillance easier, and the ability of computers to track us doubles every eighteen months.

## database

### Legacy databases

- flat-file database
- hierarchical database
- network database

#### Relational database

Relational databases organize data using *tables*. Tables are structures that impose a schema on the records that they hold. Each column within a table has a *name* and a *data type*. Each row represents an individual record or data item within the table, which contains values for each of the columns.

#### NoSQL database

Modern alternatives for data that doesn't fit the relational paradigm.

- Key-value databases/key-value stores

  simple, dictionary-style lookups for basic storage and retrieval

  e.g. [Redis](https://redis.io/)

- Document databases

  Storing all of an item's data in flexible, self-describing structures
  
  e.g. [MongoDB](https://www.mongodb.com/)
  
- Graph databases

  mapping relationships by focusing on how connections between data are meaningful

  e.g. [Neo4j](https://neo4j.com/)

- Column-family databases

  databases with flexible columns to bridge the gap between relational and document databases

  e.g. [Cassandra](https://cassandra.apache.org/)

## distributed processing

breaking up computation

- interactive
- streaming
- batch

### Cluster

Networks of computers are connected and work cooperatively to achieve the same task. This is a cluster.

Clusters improve:

- Availability
- Scalability
- Performance
- Efficiency
- Costs

### Hadoop

- Based on map-reduce architecture 

- suitabble for offline processing


### Apache spark

- Includes map-reduce capabilities

- Provides real-time, in-memory processing

- faster than Hadoop